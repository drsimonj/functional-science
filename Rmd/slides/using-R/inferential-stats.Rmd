---
title: "Using R: Inferential Statistics"
author: "Simon A Jackson"
date: "Knit on `r format(Sys.time(), '%d %B, %Y')`"
output: ioslides_presentation
---

```{r setup, echo = F, warning = F}
require(knitr, quietly = TRUE)
opts_knit$set(root.dir = '../../../')
```

# Setup

## Packages

We'll be using the tidyr, dplyr, and psych packages.

```{r, eval = F}
# Install the package from CRAN (if necessary)
install.packages(c("dplyr", "tidyr", "psych"))

# Attach to your workspace
library(dplyr)
library(tidyr)
library(psych)
```
```{r, include = F}
library(dplyr)
library(tidyr)
library(psych)
```

# t.test()

## Data sets

We'll start with two data sets.

One with a between-subject design

```{r}
b <- read.csv("data/planes.csv")
b
```

## Data sets

One with a within-subject design.

```{r}
w <- read.csv("data/energy.csv")
w
```

## One Sample t-test

One sample *t*-test for a null hypothesis.  
Default settings: null = 0; significance level = .05

```{r}
t.test(b$Distance)
```

## Independent Samples (Between)

`t.test(dv ~ group)`

```{r}
t.test(b$Distance ~ b$Design)
```

## Paired Samples (Within)

`t.test(var1, var2, paired = TRUE)`

```{r}
t.test(w$Running, w$Walking, paired = TRUE)
```

# Statistical Modelling in `R`

## Two-step Process

In general, statistical modelling in R takes two steps.

1. Define and fit your model.
2. Examine the results.

## Defining the Model: The Tilde `~`

In general, statistical models are of the form  
`explain variance in Y with X1, X2, ... Xn`

This is done is `R` using formulas of the form  
`Y ~ X1 + X2 ... Xn`.

Our outcome variable appears to the left of the tilde (`~`) and our predictors appear to the right.

## Defining the Model: Combining Predictors

The predictors can be combined in a number of ways. Here are some common operations:

`Y ~ X1 + X2`  
Predict Y with X1 and X2

`Y ~ X1:X2`  
Predict Y with the interaction of X1 and X2

`Y ~ X1 + X2 + X1:X2` OR `Y ~ X1*X2`  
Predict Y with X1, X2, and their interaction.

# ANOVA - aov()

## One Way (Between)

```{r}
# Read in our data set
d <- read.csv("data/planes.csv")

# 1. Fit the model
fit <- aov(Distance ~ Paper, data = d)

# 2. Explore the results
summary(fit)
```

## One Way (Between)

```{r}
# 2. Explore the results
model.tables(fit, "means")
```

## One Way (Between)

```{r}
# 2. Explore the results
model.tables(fit, "effects")
```

## One Way (Between)

Quick comparison with `t.test()`, which should give us same result.

```{r}
t.test(Distance ~ Paper, data = d)
```

## Two-or-More Way (Between)

```{r}
# Read in our data set
d <- read.csv("data/fishing.csv")

# 1. Fit the model
fit <- aov(distance ~ rod*line*sinker, data = d)

# 2. Explore the results
summary(fit)
```

## Two-or-More Way (Between)

```{r}
# 2. Explore the results
model.tables(fit, "means")
```

## Two-or-More Way (Between)

```{r}
# 2. Explore the results
model.tables(fit, "effects")
```

## Two-or-More Way (Between)

```{r}
# Two-variable plot
plot(d$sinker, d$distance)
```

## Two-or-More Way (Between)

```{r}
# Three-variable plot
interaction.plot(d$line, d$sinker, d$distance)
```

## Within Subjects

Apart from a `t.test()`, within-subject analysis in `R` typically requires long data formats and the observation identifying (e.g., subject id) column to be a factor.

```{r}
d <- "data/energy.csv" %>%
  read.csv() %>%
  gather(exercise, energy, Running:Cycling) %>%
  mutate(exercise = factor(exercise),
         Subject = factor(Subject))
str(d)
```

## Within Subjects

```{r}
fit <- aov(energy ~ exercise + Error(Subject / exercise), data = d)
summary(fit)
```

## Mixed

In this data set, participants (`id`) had to track a Box **or** a Circle (`Shape`), four times (`trial`).

```{r}
d <- "data/tracking.csv" %>%
  read.csv() %>%
  mutate(id = factor(id))
head(d)
```

## Mixed

```{r}
# 2x(4) mixed:
# IV between: Shape
# IV within:  trial
# DV:         time
fit <- aov(time ~ Shape*trial + Error(id / trial), data = d)
```

## Mixed

```{r}
summary(fit)
```

# Correlation

## Data set

```{r}
d <- read.csv("data/physical.csv")
head(d)
```

## Base R

R has `cor()`

```{r}
cor(d)
```

## Base R

```{r}
d %>% cor %>% round(2)
```

## With `psych`

The `psych` package has `corr.test()`. Note, unlike `cor()`, this uses pairwise by default. It also gives us sample sizes and p values as separate matrices.

```{r}
corr.test(d)
```

## Home-made Function

Let's take a look at a homemade function that makes use of `corr.test()`...

```{r}
source("R/stats-functions.R")
starMatrix
```

## Home-made Function

```{r}
d %>% starMatrix()
```

## Home-made Function

Remember, it's easy to export as a table for formatting...

```{r, eval = F}
d %>% starMatrix() %>% write.csv("rMatrix.csv")
```

## Basic Plots

Scatter plot with 2 variables:

```{r}
d %>% select(Height, Mass) %>% plot()
```

## Basic Plots

Recall `pairs.panels()` from `psych`:

```{r}
d %>% pairs.panels()
```

# Linear Models/Regression - lm()

## Simple Linear Regression

Simple linear regression is a model with one predictor. So...

```{r}
fit <- lm(Mass ~ Height, data = d)
summary(fit)
```

## Basic Plots

Can add a regression line to a scatter plot with `abline()`

```{r}
fit <- lm(Mass ~ Height, data = d)
d %>% select(Height, Mass) %>% plot()
abline(fit)
```

## Basic Plots

Can interactively `identify()` points by row number...

```{r, eval = F}
d %>% select(Height, Mass) %>% plot()
d %>% select(Height, Mass) %>% identify()
```

Now click near the points to be identified, and hit escape when done.

## Multiple Regression

Multiple regression has multiple predictors...

```{r}
fit <- lm(Mass ~ Height + Bicep, data = d)
summary(fit)
```

## Multiple Regression

What if we wanted to add an interaction?

Well, first thing we need to do is center our predictors

```{r}
d <- d %>%
  mutate(Height.c = Height - mean(Height),
         Bicep.c  = Bicep - mean(Bicep))
d %>% select(Height, Bicep, Height.c, Bicep.c) %>% describe()
```

## Multiple Regression

Now we can add an interaction term.

```{r}
fit <- lm(Mass ~ Height.c + Bicep.c + Height.c:Bicep.c, data = d)
summary(fit)
```

## Hierarchical Regression

In hierarchical models, we want to examine improvements to our model as certain variables are added. For example, does the interaction term improve our model fit?

This is done by creating 2 (or more) nested models and comparing with `anova()`. Yes, this is confusing.

```{r}
fit1 <- lm(Mass ~ Height.c + Bicep.c, data = d)
fit2 <- lm(Mass ~ Height.c + Bicep.c + Height.c:Bicep.c, data = d)
anova(fit1, fit2)
```

## Predicted and Residuals

For any model, we can obtain the predicted and residual values with two functions:

```{r}
fit <- lm(Mass ~ Height + Bicep, data = d)
pred <- predict(fit)
res  <- residuals(fit)
data.frame(predicted = pred, residual = res) %>% describe()
```

## Predicted and Residuals

```{r}
plot(pred, res)
```

## Standardized Coefficients

For whatever model, if you're after the standardised coefficients ($\beta$), then attach the `lm.beta` package to your workspace.

```{r, eval = F}
# Install the package from CRAN (if necessary)
install.packages("lm.beta")

# Attach to your workspace
library(lm.beta)
```
```{r, include = F}
library(lm.beta)
```

## Standardized Coefficients

Now, we use a fit `lm()` model as an arugment to `lm.beta()`, and call `summary()`.

```{r}
fit <- lm(Mass ~ Height + Bicep, data = d)
s.fit <- lm.beta(fit)
summary(s.fit)
```

## Standardized Coefficients

dplyr style:

```{r}
d %>%
  lm(Mass ~ Height + Bicep, data = .) %>%
  lm.beta() %>%
  summary()
```

## Checking Assumptions

For any model, we can plot information to check that assumptions are satisfied:

```{r, eval = F}
fit <- lm(Mass ~ Height + Bicep, data = d)
plot(fit)
```

Use the `Enter` key to progress through plots.

# Generalized Linear Models - glm()

## `glm()` Format

```{r, eval = F}
glm(formula,
    data = data,
    family = familytype(link = linkfunction))
```

## `glm()` Format

Family	                  Default Link Function
-------------------       ------------------------------------------------
binomial	                (link = "logit")
gaussian	                (link = "identity")
Gamma	                    (link = "inverse")
inverse.gaussian	        (link = "1/mu^2")
poisson	                  (link = "log")
quasi	                    (link = "identity", variance = "constant")
quasibinomial	            (link = "logit")
quasipoisson	            (link = "log")

## Logistic Regression

As an example, we'll try logisitic regression, which uses the binomial link function when predicting a binary variable.

For example, say we have a variable indicating whether a person is in a healthy BMI range:

```{r}
d <- read.csv("data/physical.csv") %>%
     mutate(m2 = (Height / 100) ^ 2,
            BMI = Mass / m2,
            healthy = BMI > 18.5 & BMI < 24.9)
```

Healthy is a boolean variable, treating TRUE as 1 and FALSE as 0.

## Logistic Regression

Can the size of certain body parts predict whether a person is within the healthy BMI range?

```{r}
fit <- glm(healthy ~ Bicep + Head, data = d, family = binomial())
summary(fit)
```

## Logistic Regression

Take a quick look:

```{r}
d %>% select(Bicep, healthy) %>% plot()
```